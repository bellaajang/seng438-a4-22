**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 – Mutation Testing and Web app testing**

| Group \#:      |   22  |
| -------------- | --- |
| Student Names: |     |
|    Suyoung            |     |
|    Joseph            |     |
|    Joan            |     |
|    Raghav            |     |

# Introduction

The goal of this lab was to enhance the quality and completeness of our test suite by applying mutation testing to the test cases for Range and DataUtilities from Assignment 3. Using PIT, we injected mutants into the source code and analyzed it for mutation coverage. Additionally, the assignment aimed to introduce GUI testing tools for testing one of the provided websites. For this, we used Selenium to record and replay GUI test scripts.

# Analysis of 10 Mutants of the Range class
1. Mutation on line 105: replaced double return with 0.0d  → KILLED

This mutation is in the getLowerBound function, and replaces the return value with 0.0. Several of our test cases kill this mutation, one of which is testGetLowerBoundForFiniteValue(), where the Range has a lower bound of -10 and asserts that the return value is equal to -10, so when the mutation changes the return value to 0, the test fails and the mutant is killed.

2. Mutation on line 114: Incremented (a++) double field upper → SURVIVED

This mutation is in the getUpperBound function, and after the value of this.upper is returned it increments the value. This mutation is not caught by any of our test cases because we have no test that checks that the upper bound of the Range remains unchanged after the function is executed.

3. Mutation on line 123: Replaced double subtraction with addition → KILLED

This mutation is in the getLength function, and replaces the subtraction with addition. Several of our test cases kill this mutation, one of which is getLengthWithBothBoundsPositive(), where the range 10,20, so expected length is 10, but with the mutation the returned length is 30, thus it is not asserted equal, the test fails and the mutant is killed.

4. Mutation on line 132: Substituted 2.0 with 1.0 → KILLED

This mutation is in the getCentralValue function, and replaces the first 2 in this.lower / 2.0 + this.upper / 2.0 with a 1. The testGetCentralValue() kills this mutation, as the range is 5.0,15.0, so the expected central value is 10.0, but when the mutation happens the return value is 12.5, thus it is not asserted equal, the test fails and the mutant is killed.

5. Mutation on line 132: Incremented (++a) double field lower → KILLED

This mutation is in the getCentralValue function, the value of this.lower is incremented and then used to return. The testGetCentralValue() kills this mutation, as the range is 5.0,15.0, so the expected central value is 10.0, but when the mutation happens the return value is 10.5, thus it is not asserted equal, the test fails and the mutant is killed.

6. Mutation on line 161: Decremented (a--) double local variable number 1 → SURVIVED

This mutation is in the intersects(double b0, double b1) function, and after the value of b0 is used to evaluate the return value it decrements b0. This mutation is not caught by any of our test cases because we have no test that checks that the local variable b0 that the function used remains unchanged after the function is executed.

7. Mutation on line 197: Negated double local variable number 3 → KILLED

This mutation is in the constrain function, and negates the variable result before it is returned. Several of our test cases kill this mutation, one of which is testConstrainWithinRange(), which has a Range 5,10 and tests a constrain value of 7. The expected return value is 7, but with the mutation -7 is returned, thus it is not asserted equal, the test fails and the mutant is killed.

8. Mutation on line 217: removed conditional - replaced equality check with true → KILLED

This mutation is in the combine function, and replaces the checking if range1 is null with true. This mutation is killed by testCombineWithOverlappingRanges(), because if the first if statement is true, it completely ignores range1 in making the new range, so instead of having the new expected range of -2,7, it is only range2 so -2,3, thus it is not asserted equal, the test fails and the mutant is killed.

9. Mutation on line 448: replaced boolean return with false for → KILLED

This mutation is in the isNaNRange function, and makes the return false. This mutation is killed by testIsNaNRange(), because it has the assertion assertTrue(new Range(Double.NaN, Double.NaN).isNaNRange(), and since the return is false the test fails and the mutant is killed.

10. Mutation on line 475: replaced return value with "" → KILLED

This mutation is in the toString function, and makes the return value “”. This mutation is killed by several tests, one of which is testToStringWithZero(), which has expected = "Range[0.0,0.0]", so when the return value is “”, it is not asserted equal, the test fails and the mutant is killed. 


# Report all the statistics and the mutation score for each test class
## Range Initial Mutation Score

![Screenshot 2025-03-26 213927](https://github.com/user-attachments/assets/8006ad67-6bb6-4d0a-ad3a-d3c6252e4b50)

## Range Final Mutation Score

![Screenshot 2025-03-28 001644](https://github.com/user-attachments/assets/ebfa250b-39a0-4ea7-9aa4-94e3ffd24643)

## DataUtilities Initial Mutation Score

![Screenshot 2025-03-27 205604](https://github.com/user-attachments/assets/d397657c-aee1-4b31-b25b-a48e5b35eaa5)

## DataUtilities Final Mutation Score
![screenshot](https://github.com/user-attachments/assets/dbe14de6-35ee-4596-ba4b-8ffa0a8cec24)

# Analysis drawn on the effectiveness of each of the test classes

The effectiveness of RangeTest and DataUtilities can be compared based on their coverage and mutation scores.

RangeTest started with 100% line coverage, 70% mutation coverage, and 70% test strength. Improvements brought mutation coverage to 80%, and test strength to 80%. The mutation score suggests it is relatively robust.

DataUtilities, on the other hand, began with 100% line coverage, 91% mutation coverage, and 91% test strength. After minor refinements, these scores increased to 100%, 93%, and 93%, respectively, indicating higher initial and overall effectiveness.

In summary, DataUtilities outperformed RangeTest with greater robustness and fewer required improvements.

# A discussion on the effect of equivalent mutants on mutation score accuracy
// A discussion on the effect of equivalent mutants on mutation score accuracy including a discussion on how equivalent mutants could be detected
// Try to come up with a way that you could automatically detect a few equivalent mutants in your experimentation, for classes Range and DataUtilities. 
// If you think you have found a way, discuss it in your report, along with its benefits, disadvantages, and assumptions. Try to detect a few equivalent 
// mutants manually in classes Range and DataUtilities (to do this, you will need to investigate the mutation logs generated by the tool). 
// Report the process you followed for this part plus your findings and results in your lab report.

# A discussion of what could have been done to improve the mutation score of the test suites
To improve the mutation scores of our test suites, we adopted a systematic approach that focused on identifying and addressing surviving mutants. First, we reviewed the mutation logs generated by PIT to pinpoint which lines of code and methods had the most surviving mutants. By analyzing these areas, we designed targeted test cases to address specific mutations.

Our strategy involved dividing the two test classes. After running PIT on their respective classes, members focused on methods with the highest number of surviving mutants, writing new test cases to eliminate them. We also ensured to account for edge cases and scenarios that were missed in our initial assignment, thereby increasing test robustness.

Test cases were designed based on mutant descriptions and the mutation report. This iterative process involved repeatedly running PIT, adding test cases, and checking for improved mutation coverage until the desired scores were achieved. 

# Why do we need mutation testing? Advantages and disadvantages of mutation testing
Mutation testing is important because it helps assess the effectiveness of a test suite by introducing small, intentional faults (mutants) into the source code. It checks whether the test cases can detect these faults, helping to identify weak or missing tests. This leads to higher test quality, improved code reliability, and greater confidence in the software. Unlike coverage testing, which only measures which lines of code are executed, mutation testing evaluates how well the tests detect actual faults, highlighting that coverage alone does not ensure the overall quality of the test suite.
### Advantages:
- Helps identify weak or ineffective test cases.
- Improves the overall quality and coverage of the test suite.
- Enhances source code quality by encouraging more robust testing.
- Detects bugs early in the development cycle, reducing long-term costs.
- Fully automated tools (e.g., PIT) can streamline the mutation process.

### Disadvantages:
- Can be time-consuming, especially for large codebases.
- Requires significant computational resources to run all mutations.
- Generates equivalent mutants (mutations with no observable effect), which can be hard to detect and waste tester effort.
- May result in false positives, leading to unnecessary debugging.
- Tools like PIT may take time to generate and analyze reports.

# Explain your SELENUIM test case design process
The test case design process with Selenium was conducted by using Selenium IDE extension with FireFox. Firstly, we began identifying use-case scenarios on Gap.ca website and defined corresponding test case steps. Secondly, using Selenium, we started recording steps using the test functionalities and check if they are reproducible without modifying, by checking with playing back the recording. Some of playbacks were not reproducible, so we changed the targets to have correct element selectors or locators (XPath, CSS, ID). Finally, we validated the test cases by adding assert to verify the expected outcomes no matter how many times we run the automation scripts for the test cases.

# Explain the use of assertions and checkpoints
Verification points play a crucial role in GUI testing by allowing tests to confirm that they are proceeding correctly at various stages. This can involve checking which page the site is currently on or verifying the content of the page. By adding verification points, testers can save time by stopping a test early if it is not on the expected page or if key elements are missing. However, our group chose not to implement verification points because our tests were relatively simple and quick.

# how did you test each functionaity with different test data
Testing Various Functionalities with Different Test Data

When designing our GUI tests, we focused on verifying key user interactions on the Gap website. Each functionality was tested with different data sets to ensure proper behavior.

Functionalities Tested by Our Group:

    Login

    Changing personal information

    Gift card validation

    Cart management

    Sign out

Different Test Data:

Login:

    Login with valid credentials

    Login with invalid credentials (incorrect email and password)

Changing Personal Information:

    Change first name with valid input

    Change first name with invalid input

Gift Card Validation:

    Enter an invalid gift card number and PIN

    Enter an unregistered gift card number and PIN

Cart Management:

    Add an item to the cart

    Remove an item from the cart

Sign Out:

    Successfully sign out of the account

# Discuss advantages and disadvantages of Selenium vs. Sikulix

### Advantages:
Selenium:
- quick to check since it's an extension IDE for quick test creation
- works well with element locators (ID, XPath, CSS) to interact with web elements
- ideal for web applications – supports most modern browser like FireFox

Sikulix:
- uses image recognition to interact with any part of the screen – not limited to web apps
- good while playing games you want to automate some aspects
- useful when developing an application you want to do visual testing
- good for visual validation or testing where GUI feedback is crucial.

### Disadvantages:
Selenium:
- can be brittle if the DOM structure changes frequently.
- limited to browser-based automation – cannot interact with non-web elements or desktop apps

Sikulix:
- generally slower than Selenium due to image processing and screen capture
- mainly relies on picture recognition
- image-based approach can fail in complex systems or major UI changes

# How the team work/effort was divided and managed
The work was divided evenly among the team. The work for the mutation testing was done in 2 groups - Joan and Bella on Range, and Raghav and Joseph on DataUtilities. Each group made test cases to ensure that the mutation coverage was higher than the original suite. Afterwards we shared the new test suites and how they improved from the last. For the GUI Testing, each person was responsible for 2 different test cases. We all recorded our scripts and reviewed them with one another to ensure we all had the test cases we needed as per our test plan. 

# Difficulties encountered, challenges overcome, and lessons learned
One of the challenges was understanding the mutation report outputs and identifying equivalent mutants, which made improving the mutation score more time-consuming. Additionally, running the mutation tests took an average of 6 minutes each time, which significantly slowed down our workflow. Every time we modified the code or added new test cases, we had to wait several minutes to see if the mutation score had improved. This made the iterative testing process quite tedious and time-consuming.

For GUI testing with Selenium, several recorded test scripts failed on playback due to dynamic element IDs and poorly captured selectors. We resolved this by updating our target selectors using more reliable XPath or CSS selectors and by removing unnecessary commands such as mouseOver or mouseOut.

Through these challenges, we learned the importance of thoroughly analyzing mutation results, writing targeted test cases, and refining test scripts for consistency. We also gained valuable experience in debugging Selenium scripts and learned how to make them more robust for real-world applications. Overall, the lab helped strengthen our understanding of mutation testing and automated GUI testing.

# Comments/feedback on the lab itself
The instructions for this lab were often confusing. Additionally, when we encountered issues with Eclipse or the lab setup, it would have been helpful 
to include a note in the assignment markdownabout the need to import jar files from Assignment Two, and explaining how to import the sample test suites.
